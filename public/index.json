[
{
	"uri": "/analytics201/intro/",
	"title": "소개",
	"tags": [],
	"description": "",
	"content": " 이 워크샵을 통해 AWS 기반의 데이터 분석을 실습합니다.\n준비사항  AWS 계정: EC2, S3, IAM, Kinesis(Data Analytics, Data Firehose), Glue, Athena, QuickSight, Redshift 자원을 생성할 수 있는 권한이 필요합니다. AWS 리전: 이번 실습은 서울 (ap-northeast-2) 리전에서 실행합니다. 브라우저: 최신 버전의 크롬, 파이어폭스를 사용하세요.  Contact Us AWS 서비스에 관한 질문은 AWS Support나 담당 AM을 통해서 문의해 주시고 본 워크샵의 발표자료에 관한 질문 사항은 아래의 email 링크를 통해 문의해 주시면 감사하겠습니다.\n© 2019 Amazon Web Services, Inc. 또는 자회사, All rights reserved. "
},
{
	"uri": "/analytics201/",
	"title": "홈",
	"tags": [],
	"description": "",
	"content": " AWS Analytics 서비스 워크샵 본 프로그램은 AWS의 Analytic 서비스를 실제 실습할 수 있는 강의 교제 제공을 목표로 하고 있습니다. © 2019 Amazon Web Services, Inc. 또는 자회사, All rights reserved. "
},
{
	"uri": "/analytics201/lab0/",
	"title": "실습0. 사전 준비",
	"tags": [],
	"description": "",
	"content": " 본격적인 Lab 시작에 앞서 구성에 필요한 IAM User, EC2, S3를 생성 및 구성합니다. Redshift 실습은 생략할 예정임으로 DBeaver는 설치하실 필요가 없습니다.\nIAM User 생성 Lab 전체에서 사용할 IAM User를 생성합니다.\n AWS Management Console에 로그인 한 뒤 IAM 서비스에 접속합니다. 왼쪽 메뉴에서 Users를 선택합니다.  Add user 버튼을 클릭하여 사용자 추가 페이지로 들어갑니다. User name에 \u0026lt;원하는 사용자 이름\u0026gt; 을 입력하고, Access type에 Programmatic access와 AWS Management Console access 둘 모두를 선택합니다. Console password에 \u0026lt;원하는 패스워드\u0026gt;를 입력하고, 마지막 Require password reset의 체크는 해제합니다.  Next: Permissions 버튼을 클릭하고 Attach existing policies directly를 선택한 뒤 AdministratorAccess 권한을 추가해줍니다.  Next: Review 버튼을 클릭하고 정보를 확인한 뒤 Create user 버튼을 클릭하여 사용자 생성을 완료합니다.  Download.csv 버튼을 클릭하여 생성한 사용자의 정보를 다운 받습니다. EC2 설정에 꼭 필요한 파일이므로 기억하기 쉬운 위치에 저장합니다.   EC2 생성 Lab에서 데이터를 실시간으로 발생시킬 EC2 인스턴스를 생성합니다.\n AWS Management Console에서 EC2 서비스에 접속합니다. 우측 상단에서 Region은 US East (N. Virginia)를 선택합니다. Launch Instance를 선택하여 새로운 인스턴스 생성을 시작합니다.  Step 1: Choose an Amazon Machine Image (AMI) 화면에서 Amazon Linux AMI 2018.03.0 (HVM), SSD Volume Type - ami-14c5486b 를 선택합니다.  Step 2 : Choose an Instance Type 화면에서 인스턴스 타입은 t2.micro를 선택합니다. Next: Configure Instance Details 을 클릭합니다.  Step 3: Configure Instance Details 화면에서 Advanced Details을 클릭하고 아래 userdata를 복사하여 붙여 넣습니다.\n#include https://s3.amazonaws.com/immersionday-bigdata-v20180731/userdata.sh   Step 4: Add Storage 화면에서 기본값을 그대로 두고 Next: Add Tags를 클릭합니다.\n Step 5: Add Tags 화면에서 Add Tag 버튼을 한 번 클릭한 뒤, Key/Value : Name/BigDataStream 를 입력하고 Next: Configure Security Group 을 클릭합니다.  Step 6: Configure Security Group 화면에서 Security Group에 필요한 정보를 입력한 후 Review and Launch를 클릭합니다.\n Security Group Name : bastion Description : SG for bastion Type : SSH Protocol : TCP Port Range : 22 Source : 0.0.0.0/0   Step 7: Review Instance Launch 화면에서 Launch를 클릭합니다.\n EC2 Instance에 접속하기 위한 Key pair를 생성합니다. Create a new key pair를 선택하고 Key pair name은 bigdata-hol 을 입력한 후 Download Key Pair를 클릭합니다.  Key Pair를 PC의 임의 위치에 저장한 후 Launch Instances를 클릭합니다. (인스턴스 기동에 몇 분이 소요될 수 있습니다.)  (MacOS 사용자) 다운로드 받은 Key Pair 파일의 File Permission을 400으로 변경합니다.\n$ chmod 400 ./bigdata-hol.pem $ ls -lat bigdata-hol.pem -r-------- 1 ****** ****** 1692 Jun 25 11:49 bigdata-hol.pem   EC2 설정 생성한 EC2 인스턴스가 다른 AWS 리소스에 접근 및 제어할 수 있도록 다음과 같이 구성합니다.\n 생성한 인스턴스의 Public IP로 SSH 접속을 합니다.\nssh -i “\u0026lt;Key pair name\u0026gt;\u0026quot; ec2-user@\u0026lt;Public IP\u0026gt;  \u0026nbsp;* Windows OS를 사용하시는 경우 Putty를 이용하여 접속합니다.\n User Data를 통해 3가지 파일(banking_loss.csv, firehose.py, redshift.py)이 잘 다운로드 받아졌는지 확인합니다.\nls   AWS의 다른 리소스 접근을 위해 AWS Configure를 진행합니다. 이때 앞서 생성한 IAM User 데이터를 활용합니다. 이전에 다운로드 받은 .csv 파일을 열어 Access key ID와 Secret access key를 확인하고 입력합니다.\naws configure AWS Access Key ID [None]: \u0026lt;Access key ID\u0026gt; AWS Secret Access Key [None]: \u0026lt;Secret access key\u0026gt; Default region name [None]: us-east-1 Default output format [None]:   설정이 완료 되었다면 다음과 같이 입력하신 정보가 마스킹 되어 보이게 됩니다.   S3 Bucket 생성 발생한 실시간 데이터를 저장할 S3 Bucket을 생성합니다.\n AWS Management Console에서 S3 서비스에 접속합니다. Create bucket 버튼을 클릭하고 다음과 같은 정보를 입력한 뒤 Create 버튼을 클릭하여 bucket을 생성합니다.     \u0026nbsp; \u0026nbsp;     Bucket name bigdata-immersionday-[개인식별자]  (예 : bigdata-immersionday-foobar)   Region US East (N. Virginia)   그 외 default    © 2019 Amazon Web Services, Inc. 또는 자회사, All rights reserved. "
},
{
	"uri": "/analytics201/lab1/",
	"title": "실습1. Kinesis로 S3에 데이터 수집",
	"tags": [],
	"description": "",
	"content": " Lab 설명 이번 Lab은 EC2에서 발생하는 데이터를 Kinesis Firehose를 통해 수집하여 S3에 저장합니다. 그 후 Kinesis Analytics로 실시간 쿼리해 분석해보고, 결과를 S3에 다시 저장합니다.\nLab Architecture Kinesis Firehose 생성 (원본 데이터) Kinesis Firehose를 통해 앞서 생성한 EC2가 발생시키는 실시간 데이터를 S3, Redshift, ElasticSearch 등의 목적지에 수집할 수 있습니다.\n AWS Management Console에서 Kinesis 서비스를 선택합니다. (Region : N.Virginia) Get Started 버튼을 클릭합니다. Deliver streaming data with Kinesis Firehose delivery streams 메뉴의 Create delivery stream 을 클릭하여 새로운 Firehose 전송 스트림 생성을 시작합니다.  Delivery stream name에 Source 를 입력한 뒤 Next를 클릭합니다. (주의 : Source 이외의 name을 지정할 경우 EC2에서 Kinesis Firehose로 log가 전송되지 않습니다.) Record transformation / Record format conversion 은 default인 Disabled로 두고 Next를 클릭합니다. Destination은 Amazon S3를 선택하고 아래 S3 bucket은 앞서 생성한 bucket을 선택합니다. Prefix에는 source/ 를 입력합니다. Next를 클릭합니다  Buffer size는 1MB, Buffer interval은 60 seconds로 설정합니다. 아래 IAM role에서 Create new, or Choose 버튼을 클릭합니다.  새로 열린 탭에서 필요한 정책이 포함된 IAM 역할 firehose_delivery_role을 자동으로 생성합니다. Allow 버튼을 클릭하여 진행합니다.  새롭게 생성된 역할이 추가된 것을 확인한 뒤 Next 버튼을 클릭합니다.  Review에서 입력한 정보를 확인한 뒤 틀린 부분이 없다면 Create delivery stream 버튼을 클릭하여 Firehose 생성을 완료합니다.  Firehose delivery streams 가 생성 되었습니다.   데이터 수집 확인 생성한 Firehose가 정상적으로 데이터를 수집하는지 확인해봅니다.\n 앞서 생성한 EC2 인스턴스에 SSH 접속을 합니다. firehose.py 를 실행합니다.\npython firehose.py  매 초 데이터가 발생하는 것을 확인합니다. 충분한 데이터 수집을 위해 실행 중인 상태로 다음 단계를 진행합니다.\n 몇 분 뒤 생성한 S3 bucket에 가보면 생성된 원본 데이터가 Firehose를 통해 S3에 저장되는 것을 확인할 수 있습니다. 이 때 지정한 Prefix인 source 폴더에 데이터가 저장됩니다.   Kinesis Firehose 생성 (아웃풋 데이터) 앞서 생성한 Firehose 전송 스트림과 동일한 방법으로 Destination 전송 스트림을 생성합니다. 이 때, Delivery stream name과 S3 destination의 Prefix만 새로운 값을 입력합니다.\n Delivery stream name에 Destination 를 입력한 뒤 Next를 클릭합니다.  Destination은 Amazon S3를 선택하고 아래 S3 bucket은 앞서 생성한 bucket을 선택합니다. Prefix에는 destination/ 을 입력합니다. Next를 클릭합니다.  Buffer size는 1MB, Buffer interval은 60 seconds로 설정합니다. IAM Role은 앞서 생성된 firehose_delivery_role을 선택합니다. Policy Name은 Create a new Role Policy를 선택합니다.  Source, Destination 2개의 Firehose 전송 스트림 생성을 완료합니다.   Kinesis Analytics Application - 생성 Kinesis Analytics를 활용하여 스트리밍 데이터를 처리할 application을 생성합니다.\n AWS Management Console에 로그인 한 뒤 Kinesis 서비스에 접속합니다. Kinesis analytics applications 의 Create analytics application을 선택합니다.  Application name에 StreamApplication을 입력하고 Create application 버튼을 클릭합니다.   Kinesis Analytics Application - Source 지정  생성한 application 화면으로 이동 후 Connect streaming data 버튼을 클릭합니다.  Source는 Kinesis Firehose delivery stream 을 선택하고 앞서 생성한 2개의 스트림 중 Source 를 선택합니다. [Discover Schema] 버튼을 클릭하여 Schema를 분석합니다. Schema 분석 후 아래에 샘플 데이터를 보여줍니다. (Schema 탐색이 실패했다면 EC2 인스턴스에서 firehose.py 가 실행 중 인지 확인합니다)  Schema분석이 끝나면 DATE 데이터 타입을 지정해주기 위해 Edit schema 버튼을 클릭합니다.  OccurrenceStartDate와 DiscoveryDate의 타입을 DATE로 변경해줍니다. NetLoss, RecoveryAmount, EstimatedGrossLoss는 DOUBLE로 변경해줍니다. Save schema and update stream samples 버튼을 클릭하여 저장합니다.  저장이 완료되면 Exit(done) 버튼을 클릭한 뒤, Save and continue 버튼을 클릭하여 Source 지정을 완료합니다.   Kinesis Analytics Application - SQL 실시간 쿼리  Go to SQL editor 메뉴로 이동합니다.  Text editor에 다음과 같은 SQL문을 입력한 뒤 Save and run SQL 버튼을 클릭합니다. (이 때 Source data가 없다는 문구가 표시된다면 EC2에서 firehose.py가 실행 중인지 확인 후 Refresh 합니다.)\nCREATE OR REPLACE STREAM \u0026quot;DESTINATION_SQL_BASIC_STREAM\u0026quot; ( Region VARCHAR(16), Business VARCHAR(32), Name VARCHAR(16), Status VARCHAR(16), RiskCategory VARCHAR(64), RiskSubCategory VARCHAR(64), DiscoveryDate DATE, OccurrenceStartDate DATE, NetLoss DOUBLE, RecoveryAmount DOUBLE, EstimatedGrossLoss DOUBLE); CREATE OR REPLACE PUMP \u0026quot;STREAM_PUMP_1\u0026quot; AS INSERT INTO \u0026quot;DESTINATION_SQL_BASIC_STREAM\u0026quot; SELECT STREAM \u0026quot;Region\u0026quot;, \u0026quot;Business\u0026quot;, \u0026quot;Name\u0026quot;, \u0026quot;Status\u0026quot;, \u0026quot;RiskCategory\u0026quot;, \u0026quot;RiskSubCategory\u0026quot;, \u0026quot;DiscoveryDate\u0026quot;, \u0026quot;OccurrenceStartDate\u0026quot;, \u0026quot;NetLoss\u0026quot;, \u0026quot;RecoveryAmount\u0026quot;, \u0026quot;EstimatedGrossLoss\u0026quot; FROM SOURCE_SQL_STREAM_001\u0026quot;;  SQL 쿼리가 실시간으로 수행되는 것을 확인할 수 있습니다. Close 버튼을 클릭합니다.   Kinesis Analytics Application - Destination 지정  Kinesis Analytics의 SQL 수행 결과를 S3에 저장할 수 있습니다. Kinesis Analytics StreamApplication 화면으로 돌아가 Connect to a destination 을 클릭합니다.    Destination : Kinesis Firehose delivery stream Kinesis Firehose delivery stream : Destination In-application stream name : DESTINATION_SQL_BASIC_STREAM, Output format : JSON Access to chosen resources : Create update IAM role kinesis-analytics-StreamApplication-us-east-1 을 지정한 후 Save and Continue 버튼을 클릭합니다.   Destination이 설정 되었습니다. Exit to Kinesis Analytics applications 을 클릭합니다.  EC2에서 firehose.py 실행을 중단했다면 다시 실행합니다. 몇 분 뒤 S3 bucket을 보면 설정한 Prefix인 destination 폴더가 생성된 것을 확인할 수 있습니다. 해당 폴더에는 설정한대로 JSON 포맷으로 데이터가 수집됩니다.   © 2019 Amazon Web Services, Inc. 또는 자회사, All rights reserved. "
},
{
	"uri": "/analytics201/lab2/",
	"title": "실습2. Glue, Athena, Quicksight로 수집한 S3의 데이터 분석",
	"tags": [],
	"description": "",
	"content": " Lab 소개 이번 Lab에서는 이전 시간에 진행한 Lab에서 Kinesis Firehose로 S3에 저장한 데이터를 Glue, Athena, Quicksight 를 이용해 분석해봅니다.\nLab Architecture AWS Glue AWS Glue는 완전관리형 ETL(Extract, Transform, Load) 엔진입니다. 실습에서는 AWS Glue로 Kinesis Firehose가 S3에 저장한 데이터를 크롤링하여 Hive 호환 External Table을 만들고 Athena, QuickSight로 데이터를 분석하고 시각화 합니다.\n AWS Glue에서 ETL Job을 수행하기 위해서는 특정 IAM Role이 필요합니다. IAM 서비스에 접속합니다. https://console.aws.amazon.com/iam 좌측 메뉴에서 Policy를 클릭하고 Create Policy 버튼을 클릭합니다. Create Policy 화면에서 JSON 탭을 클릭하고 아래 Policy를 복사하여 붙여 넣고 Review Policy를 클릭합니다.\n{ \u0026quot;Version\u0026quot;: \u0026quot;2012-10-17\u0026quot;, \u0026quot;Statement\u0026quot;: [ { \u0026quot;Action\u0026quot;: [ \u0026quot;s3:*\u0026quot;, \u0026quot;ec2:*\u0026quot;, \u0026quot;iam:*\u0026quot;, \u0026quot;glue:*\u0026quot;, \u0026quot;logs:*\u0026quot; ], \u0026quot;Resource\u0026quot;: \u0026quot;*\u0026quot;, \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot; } ] }  Review Policy 화면에서 Name은 GluePolicy를 입력하고 Create Policy 버튼을 클릭합니다.\n 좌측 메뉴에서 Roles을 클릭하고 Create Role 버튼을 클릭합니다.\n Choose the service that will use this role 에서 Glue를 클릭하고 Next: Permission 버튼을 클릭합니다.\n Search 입력창에 방금 만든 GluePolicy를 입력하고 GluePolicy를 선택하고 Next: Review 버튼을 클릭합니다.\n Review 화면에서 Role name은 GlueRole을 입력하고 Create Role버튼을 클릭합니다.\n 좌측 Roles 을 클릭하고 GlueRole이 정상적으로 생성되었는지 확인합니다.\n AWS Glue (https://console.aws.amazon.com/glue 로 이동합니다. 이전 Lab에서 생성한 S3내 source 폴더 아래 파일 들을 Crawling하여 메타스토어 테이블을 만듭니다. Data catalog \u0026gt; Databases 를 클릭하고 Add database 를 클릭합니다.  Database name 은 Immersion Day 를 입력하고 Create 를 클릭합니다.\n Data catalog \u0026gt; Databases \u0026gt; Tables 를 클릭하고 Add tables \u0026gt; Add tables using a crawler 를 클릭합니다.  Crawler name에 TestCrawler 입력 후 Next 를 클릭합니다.\n Crawling 할 S3 Bucket 및 폴더를 지정합니다. Include Path 에 s3://bigdata-immersionday/source 을 입력하거나 탐색기 버튼을 클릭하여 bigdata-immersionday 버킷 아래 source 폴더를 선택한 후 Next를 클릭합니다.\n Add another data store는 No를 선택하고 Next를 클릭합니다.\n Choose an IAM role 화면에서 Choose an existing IAM role을 선택하고 앞서 생성한 GlueRole을 선택한 후 Next를 클릭합니다.\n Crawler는 on-demand 방식으로 수행할 수 있고, 배치 방식으로 수행할 수 있습니다. 실습에서는 ondemand 방식으로 수행합니다. Frequency는 Run on demand를 선택하고 Next를 클릭합니다.\n Database는 앞서 생성한 Immersion Day를 선택한 후 Next를 클릭합니다.\n Crawler 설정을 모두 확인한 후 Finish를 클릭합니다.\n Data catalog \u0026gt; Crawlers 에서 생성한 TestCrawler를 선택한 후 Run crawler를 클릭합니다.  Crawler는 S3에 저장된 파일을 분석하고 테이블을 생성합니다. Crawling이 끝난 후 테이블 1개 (테이블명 : source)가 생성되었음을 확인합니다.  Databases \u0026gt; Tables 에서 방금 생성된 source 테이블을 클릭하여 테이블 구조를 확인합니다.\n Kinesis Firehose가 s3://bigdata-immersionday/source 에 저장한 JSON 파일 포맷 스트림 데이터를 Parquet 파일 포맷으로 변경하고, 일부 컬럼의 데이터 타입을 변경한 후 s3://bigdataimmersionday/parquet 폴더에 파일을 저장하는 Glue ETL 작업을 생성하겠습니다.\n 우선 ETL후 결과 파일이 저장될 폴더를 생성합니다. S3 콘솔로 로그인한 후 bigdata-immersionday 버킷를 선택하고 parquet 폴더를 생성합니다.  Glue 콘솔(https://console.aws.amazon.com/glue)에서 ETL \u0026gt; Jobs를 선택한 후 Add Job을 클릭합니다.\n Job Properties 화면에서 Name은 TestJob을 입력하고, IAM role은 앞서 생성한 GlueRole을 선택합니다. Advanced properties 에서 Job bookmark를 Enable하여 Glue가 마지막으로 처리한 데이터를 기억하게 합니다. Next를 클릭합니다.\n Choose your data sources 화면에서 source를 선택하고 Next를 클릭합니다.  Choose your data targets 화면에서 Create tables in your data target를 클릭하고 Data store는 Amazon S3, Format은 Parquet, target bucket은 s3://bigdata-immersionday/parquet 를 입력합니다. Next를 클릭합니다.  occurencestartdate, discoverydate 컬럼의 data type을 DATE로 변경한 후 Next를 클릭합니다.  Save job and edit script 버튼을 클릭합니다.\n 스크립트 내용을 검토한 후 상단의 Run Job버튼을 클릭하여 ETL을 시작합니다. 작업 완료까지 수 분이 소요될 수 있습니다.\n Glue ETL 작업이 끝나면 s3://bigdata-immersionday/parquet 폴더에 parquet 타입 파일이 생성됩니다.\n Glue ETL이 변환한 Parquet 파일을 Crawling하여 테이블을 생성합니다. AWS Glue \u0026gt; Crawler \u0026gt; TestCrawler를 선택하고 Action \u0026gt; Edit Crawler 를 선택합니다.  Add information about your crawler 화면에서 Next 를 클릭합니다.\n Add a data store 화면에서 Next 를 클릭합니다.\n Add another data store 화면에서 Yes를 선택한 후 Next를 클릭합니다.\n Add a data store 화면에서 Include path에 s3://bigdata-immersionday/parquet 를 입력한 후 Next를 클릭합니다.  Add another data store 화면에서 Next를 클릭합니다.\n Choose an IAM role 화면에서 Next를 클릭합니다.\n Create a schedule for this crawler 화면에서 Next를 클릭합니다.\n Configure the crawler’s output 화면에서 Next를 클릭합니다.\n Review 화면에서 Finish를 클릭합니다.\n TestCrawler를 선택하고 Run crawler를 클릭합니다.  Crawler 실행 후 parquet 테이블이 생성됩니다.   Amazon Athena  Athena를 이용하여 테이블 데이터를 조회할 수 있습니다. Glue Crawler가 만든 parquet 테이블을 클릭하고 Action \u0026gt; View data를 클릭합니다.  Athena Console이 열리고 SELECT * FROM \u0026ldquo;immersion day\u0026rdquo;.\u0026ldquo;parquet\u0026rdquo; limit 10; Query가 수행되었습니다.  ANSI 표준 SQL문을 통해 S3내 데이터를 조회할 수 있습니다.\nSELECT region, status, count(*) as \u0026quot;COUNT\u0026quot; FROM \u0026quot;immersion day\u0026quot;.\u0026quot;parquet\u0026quot; GROUP BY region, status ORDER BY region;   Amazon QuickSight  이번에는 Amazon Quicksight를 통해 parquet 테이블 데이터를 시각화 해 보도록 하겠습니다. Quicksight 콘솔로 이동합니다. https://quicksight.aws.amazon.com Quicksight에 가입하기 위해 Sign up for QuickSight 버튼을 클릭합니다.  Standard Edition을 선택한 후 Continue버튼을 클릭합니다. Quicksight account name은 임의로 지정(중복될 경우 계정이 생성되지 않습니다) 하고 Notification email address는 개인 Email 주소를 입력합니다. QuckSight가 S3에 Access해야 하므로, Choose S3 buckets를 클릭하여 bigdata-immersionday를 선택한 후 Finish를 클릭합니다.  계정이 생성된 후 Go to Amazon Quicksight 버튼을 클릭합니다. 좌측 상단 New Analysis를 클릭합니다.  New Data Set 버튼을 클릭합니다.  Athena를 클릭하고 팝업 창의 Data source name에 bigdata-quicksight를 입력(임의의 값 입력 가능)하고 Create data source버튼을 클릭합니다.  Choose your table에서 Database는 Immersion day, Tables는 parquet를 선택하고 Select 버튼을 클릭합니다.  Visualize 버튼을 클릭한 후 parquet 테이블 데이터가 Quicksight SPICE 엔진에 로딩 되었는지 확인합니다.  발생년도 별 Business Count를 시각화 해 보겠습니다. 좌측 Fields list에서 occurrencestartdate, business field를 차례대로 클릭합니다. Visual types는 세로 막대 그래프를 선택합니다.  그래프 하단 occurrencestartdate 를 클릭하고 Aggregate: Day를 Year로 변경합니다.  연도별로 데이터가 집계 되었습니다.  방금 만든 Dashboard를 다른 사용자에게 공유해 보겠습니다. 좌측 상단 유저 아이콘을 클릭하고 Manage QuickSight를 클릭합니다. Invite users 버튼을 클릭한 후 임의의 사용자 계정명(BI_user01)을 입력한 후 우측 [+] 버튼을 클릭합니다. Email은 다른 사용자의 Email 주소를 입력하고 Role은 AUTHOR, IAM User는 NO를 선택한 후 Invite 버튼을 클릭합니다.  사용자는 다음과 같은 Invitation Email을 받고 Click to accept invitation을 클릭하면 계정 생성 메뉴에서 비밀번호를 변경할 수 있습니다.  QuickSight 화면으로 돌아가서 우측 상단의 Share \u0026gt; Share analysis를 클릭합니다.  BI_user01을 선택한 후 Share 버튼을 클릭합니다.  사용자는 다음과 같은 Email을 수신합니다. Click to View를 클릭하여 분석결과를 확인할 수 있습니다.   © 2019 Amazon Web Services, Inc. 또는 자회사, All rights reserved. "
},
{
	"uri": "/analytics201/lab3/",
	"title": "실습3. EMR을 이용한 Hadoop 클러스터 구성 및 데이터 분석",
	"tags": [],
	"description": "",
	"content": " Amazon EMR은 관리형 Hadoop 프레임워크로서 빠르게 빅데이터 분석을 위한 Hadoop 클러스터 구성을 할 수 있습니다.\nEMR 클러스터 생성 EMR 클러스터를 구성합니다. 본 Lab에서는 Spark와 Zeppelin을 활용합니다. 1. AWS Management Console에 로그인 한 뒤 N.Virginia Region의 EMR 서비스에 접속합니다. 2. Create cluster 버튼을 선택하여 클러스터 구성을 시작합니다. 3. Go to advanced options 버튼을 선택하여 원하는 애플리케이션을 직접 선택하여 구성합니다. 4. Release는 emr-5.25.0, 소프트웨어는 Spark 2.4.3과 Zeppelin 0.8.1를 선택하고 “Use for Hive table metadata” 및 “Use for Spark table metadata”를 선택합니다. 5. 하드웨어 구성 단계에서는 Network 선택에서 vpc-xxxxx 를 선택하고 EC2 Subnet에서는 가장 상단의 항목을 선택합니다. 나머지 항목은 그대로 두고 Next 를 클릭합니다. 6. Cluster name에 원하는 클러스터 이름을 입력하고, Termination protection 옵션은 체크 해제 합니다. Next를 클릭합니다. 7. Security Options에서 앞에서 생성한 EC2 key pair를 선택합니다. 8. 다음과 같이 두 단계로 Security를 설정합니다. 1 단계) EC2 Security Groups 섹션에서 Master와 Core\u0026amp;Task의 EMR managed security groups 에는 모두 각각 default 그룹을 설정하고 2 단계) Master 노드의 Additional security groups에는 사전 준비에서 생성했던 security group (Port 22 허용)을 추가로 할당합니다. 9. 화면 하단의 Create cluster를 선택하여 클러스터를 생성합니다. EMR 클러스터 생성에 약 10분~15분 정도 소요됩니다. 10. 클러스터의 Status가 Waiting (Cluster ready)이 되면 다음 Lab을 진행합니다. EMR Web Connection 설정 (Windows OS)  생성한 클러스터를 선택하여 Master DNS 정보를 확인합니다.  SSH의 Tunnels에서 Source port에 8157을 Destination에는 localhost:8890 을 입력한 뒤 Add를 클릭합니다.  Auth에는 EMR 클러스터 생성시에 사용한 Key pair의 .ppk 키를 추가합니다. 마지막으로 Session의 Host Name에는 다음을 입력한 뒤 Open을 클릭하여 접속합니다.\nhadoop@\u0026lt;Master Public DNS\u0026gt;   브라우저에서 http://localhost:8157 주소로 접속합니다.   EMR Web Connection 설정 (Mac OS / Linux OS)  간단히 마스터 노드와의 SSH 터널을 만들어 접근할 수 있습니다. Master public DNS를 확인합니다.  터미널에서 다음 명령어를 입력합니다. 응답을 반환 하지는 않습니다.\nssh -i “\u0026lt;Key pair name\u0026gt;” -N -L 8157:localhost:8890 hadoop@\u0026lt;Master public DNS\u0026gt;   브라우저에서 http://localhost:8157 주소로 접속합니다.   Zeppelin을 활용한 빅데이터 분석  Zeppelin Tutorial 을 클릭합니다.  여러분은 S3 (EMRFS)에 저장된 데이터(각 필드가 ‘;’로 구분)로 부터 6개의 컬럼을 읽어 필요한 자료형을 적용하고 이를 임시 테이블로 저장하는 코드를 볼 수 있습니다.  원본 코드에서 추가로 몇몇 컬럼을 더 가져오기(6번째 12번째 컬럼을 추가로 활용) 위하여 https://github.com/setch3000/emr-s3/blob/master/sample.scala 에서 샘플 코드를 복사하여 Zepplelin notebook의 Cell에 붙여 넣기를 한 후 ▷버튼을 눌려 다시 한번 실행 해 봅니다. %spark import org.apache.commons.io.IOUtils import java.net.URL import java.nio.charset.Charset //Load Bank Data val bankText = sc.parallelize( IOUtils.toString( new URL(\u0026quot;https://s3.amazonaws.com/apache-zeppelin/tutorial/bank/bank.csv\u0026quot;), Charset.forName(\u0026quot;utf8\u0026quot;)).split(\u0026quot;\\n\u0026quot;)) case class Order(age: Integer, job: String, marital: String, education: String, amount: Integer, housing: String, campaign: String) val bank = bankText.map(s =\u0026gt; s.split(\u0026quot;;\u0026quot;)).filter(s =\u0026gt; s(0) != \u0026quot;\\\u0026quot;age\\\u0026quot;\u0026quot;).map( s =\u0026gt; Order(s(0).toInt, s(1).replaceAll(\u0026quot;\\\u0026quot;\u0026quot;, \u0026quot;\u0026quot;), s(2).replaceAll(\u0026quot;\\\u0026quot;\u0026quot;, \u0026quot;\u0026quot;), s(3).replaceAll(\u0026quot;\\\u0026quot;\u0026quot;, \u0026quot;\u0026quot;), s(5).replaceAll(\u0026quot;\\\u0026quot;\u0026quot;, \u0026quot;\u0026quot;).toInt, s(6).replaceAll(\u0026quot;\\\u0026quot;\u0026quot;, \u0026quot;\u0026quot;), s(12).replaceAll(\u0026quot;\\\u0026quot;\u0026quot;, \u0026quot;\u0026quot;) ) ).toDF() bank.registerTempTable(“bank\u0026quot;) bank.show()  로딩 된 데이터를 바탕으로 다양한 분석을 진행 해 봅니다. 4.1 결혼 여부 / 주택 소유 여부에 따른 평균 이용 금액\n%sql select marital,housing, avg(amount) from bank group by marital, housing  4.2 직업별 평균 이용 금액\n%sql select job, avg(amount) from bank group by job order by 2  4.3 20대 나이별 평균 이용 금액\n%sql select age, avg(amount) from bank where age \u0026gt; ${minAge=19} and age \u0026lt; ${maxAge=30} group by age  4.4 연령/교육 수준별 이용 금액\n%sql select age, amount, education, housing from bank  4.5 직업 별 평균 마케팅 노출 횟수\n%sql select job, avg(campaign) as campaign from bank group by job  4.6 결혼 직업 별 이용 금액 합계\n%sql select marital, job, sum(amount) from bank group by marital, job   분석 결과를 그래프로 표현 하면 아래와 같이 원하는 정보를 같은 방식으로 분석할 수 있습니다.   Glue Data Catalog를 활용하여 S3에 저장된 데이터 분석 EMR 클러스터에서 Glue의 카탈로그를 조회하여 손쉽게 S3에 저장된 데이터를 분석 할 수 있습니다. 1. 화면 좌측 상단의 Zeppelin 이미지를 클릭하여 Zeppelin의 첫페이지로 이동합니다. 2. Create new note 버튼을 클릭 합니다. 3. ’TEST’라는 이름으로 새 노트를 생성 합니다. 4. Glue의 카탈로그를 조회하여 데이터베이스 리스트를 확인 합니다.\n%spark spark.sql(\u0026quot;show databases\u0026quot;).show()  5. 확인된 데이터베이스 리스트 중 “immersion day”의 테이블들을 확인해 봅니다.\n%spark spark.catalog.setCurrentDatabase(\u0026quot;immersion day\u0026quot;) spark.sql(\u0026quot;show tables\u0026quot;).show()  6. 다음과 같이 쿼리를 수행하여 데이터 분석을 진행 합니다.\n%sql select region, status, count(*) from parquet group by region, status order by region  Option : EMR 클러스터에 노드 추가 EMR 클러스터에서 Hadoop의 슬레이브 노드에 해당하는 작업을 실행하는 Core 노드와 Task 노드를 쉽게 추가할 수 있습니다. 1. AWS Management Console에 로그인 한 뒤 EMR 서비스에 접속합니다. 2. 생성한 클러스터를 선택한 후, Hardware 탭을 선택합니다. 3. Core의 Instance Count의 Resize 버튼을 클릭하여 3 으로 변경합니다. 4. Add task Instance group을 클릭하여 Task 노드를 1개 추가합니다. 5. Core와 Task 노드의 크기 변경이 시작됩니다. 각 노드에 Auto Scaling을 적용할 수도 있습니다. © 2019 Amazon Web Services, Inc. 또는 자회사, All rights reserved. "
},
{
	"uri": "/analytics201/lab4/",
	"title": "실습4. AWS 기반 데이터 웨어하우징 - Amazon Redshift",
	"tags": [],
	"description": "",
	"content": " Redshift 을 위한 IAM Role 설정  AWS Management Console에 로그인 한 뒤 IAM 서비스에 접속합니다. Roles를 클릭하고 Create new role을 클릭합니다. Select type of trusted entity 는 AWS service 를 선택하고 Choose the service that will use this role 은 Redshift를 선택합니다. 마지막으로 Select your use case 는 Redshiftcustomizable을 선택한 후 Next: Permissions 버튼을 클릭합니다.  생성할 Role에는 두 개의 Policy를 Attach 합니다. Filter에 s3 를 입력하고 AmazonS3FullAccess 정책을 체크한 후, 다시 Filter에 Glue를 입력하고 AWSGlueConsoleFullAccess 정책을 체크한 후 Next: Tags를 클릭 후, 다음 Next: Review 버튼을 클릭합니다.  Role name에 redshift_role을 입력하고 Create role을 클릭하여 IAM Role 생성을 완료합니다. 차후 실습에 사용을 위해 Role ARN을 기록합니다. (arn:aws:iam:: 으로 시작)   Amazon Redshift 클러스터 생성 실습을 위하여 Redshift 클러스터를 생성합니다. 클러스터는 샘플 데이터가 있는 us-west-2 (Oregon) 와 동일한 리전에 생성해야 합니다. 또한 Redshift 의 접속을 위하여 보안 그룹 설정을 주의하여 생성 하시기 바랍니다. \n AWS Management Console에서 Redshift 서비스에 접속 후 좌측 Clusters 탭을 선택 합니다. Launch cluster 버튼을 클릭하여 클러스터 생성을 시작합니다. Cluster identifier, Database name, Master user name, Master user password, Confirm password를 임의대로 차례로 입력한 뒤 Continue를 클릭합니다. (Database Port는 Default Port인 5439 사용합니다.)  Cluster identifier : redshift Database name : redshift Master user name : admin Password: **** (임의의 Redshift Master용 password 생성)   Node Configuration 부분은 default 옵션으로 진행합니다. Continue를 클릭합니다.  Additional Configuration에서 VPC security groups에는 default를 선택합니다. Available roles에는 이전 단계에 생성한 redshift_role을 선택합니다. Continue를 클릭합니다.  Review한 뒤 Launch cluster를 선택하여 클러스터를 생성합니다. Cluster Status가 available이 되면 다음으로 진행합니다.  Redshift 접속 이 과정에서는 Amazon Redshift 클러스터에 연결합니다. 저희 실습에서는 Redshift에서 기본으로 제공하는 쿼리 작성기인 Query editor를 이용합니다. 본 도구는 임시적으로 사용하는 도구이며, 실제로는 JDBC 연결을 통해 \u0026nbsp; 적절한 데이터베이스 관리도구를 이용 \u0026nbsp;하게 됩니다.\n Redshift 관리 콘솔에서 Query editor를 실행 합니다.  다음 설정을 구성합니다.  Cluster: redshift Database: redshift Username: admin Password: (이전 단계에서 만든 password 입력) Connect 를 클릭   외부 테이블 생성 이 실습에서는 외부 테이블을 생성합니다. 일반 Redshift 테이블과는 달리 외부 테이블은 Amazon S3에 저장된 데이터를 참조합니다.  먼저 외부 스키마를 정의합니다. 외부 스키마는 외부 데이터 카탈로그에 있는 데이터베이스를 참조하고, 클러스터가 사용자 대신 Amazon S3에 액세스할 수 있도록 권한을 부여하는 IAM 역할 식별자(ARN)를 제공합니다. \n \u0026nbsp; INSERT-YOUR-REDSHIFT-ROLE \u0026nbsp;을 실습 2에서 생성한 redshift_role의 ARN값으로 대체하고 Query editor에서 이 명령을 실행합니다.\nCREATE EXTERNAL SCHEMA spectrum FROM DATA CATALOG DATABASE 'spectrumdb' IAM_ROLE 'INSERT-YOUR-REDSHIFT-ROLE' CREATE EXTERNAL DATABASE IF NOT EXISTS  Query editor의 결과는 별도 정보가 표시되지 않고 \u0026ldquo;No records found\u0026rdquo;라는 메시지를 수신합니다.  Schema \u0026ldquo;spectrum\u0026rdquo; already exists라는 메시지를 수신하면, 다음 단계로 진행하십시오.  이제 spectrum 스키마에 저장될 외부 테이블을 생성합니다.  Query editor에서 이 명령을 실행하여 외부 테이블을 생성합니다.\nCREATE EXTERNAL TABLE spectrum.sales( salesid INTEGER, listid INTEGER, sellerid INTEGER, buyerid INTEGER, eventid INTEGER, dateid SMALLINT, qtysold SMALLINT, pricepaid DECIMAL(8,2), commission DECIMAL(8,2), saletime TIMESTAMP ) ROW FORMAT DELIMITED FIELDS TERMINATED BY '\\t' STORED AS TEXTFILE LOCATION 's3://id-redshift-uswest2/tickit/spectrum/sales/' TABLE PROPERTIES ('numRows'='172000')  Query editor에는 아무런 정보가 표시되지 않습니다. 외부 테이블은 테이블의 목록에 표시되지 않기 때문입니다. 이 문이 Amazon S3에 있는 디렉터리를 가리키는 테이블 정의를 생성했습니다. 디렉터리에는 172,456개의 행이 있는 11MB 텍스트 파일 1개가 포함되어 있습니다. 다음은 파일 콘텐츠 샘플입니다.\n2 4 8117 11498 4337 1983 2 76.00 11.40 2008-06-06 05:00:16 6 10 24858 24888 3375 2023 2 394.00 59.10 2008-07-16 11:59:24 7 10 24858 7952 3375 2003 4 788.00 118.20 2008-06-26 00:56:06 8 10 24858 19715 3375 2017 1 197.00 29.55 2008-07-10 02:12:36  각 줄에는 수량, 가격 및 판매 날짜와 같은 판매 정보가 있습니다.\n  Amazon S3에 저장된 데이터를 쿼리 이 실습에서는 외부 테이블에 대해 쿼리를 실행합니다. 이 쿼리는 Redshift Spectrum을 사용하여 Amazon S3에서 바로 데이터를 처리합니다.\n 다음 명령을 실행하여 S3에 저장된 행의 수를 쿼리합니다.\nSELECT COUNT(*) FROM spectrum.sales  출력값은 파일에 172,456 개의 레코드가 있음을 보여줍니다.\n 다음 명령을 실행하여 외부 테이블에 저장된 데이터 샘플을 확인합니다.\nSELECT * FROM spectrum.sales LIMIT 10  S3에 저장된 탭으로 분리된 데이터가 일반 Redshift 테이블과 정확히 동일하게 표시되는 것을 확인할 수 있습니다. Spectrum은 S3에서 데이터를 읽지만 마치 Redshift가 직접 읽는 것처럼 표시합니다. 또한, 쿼리는 합계 계산과 같은 일반 SQL 문을 포함할 수 있습니다.  다음 명령을 실행하여 일의 매출을 계산합니다.\nSELECT SUM(pricepaid) FROM spectrum.sales WHERE saletime::date = '2008-06-26'   Amazon Redshift Spectrum은 임시 Amazon Redshift 테이블로 데이터를 로드할 필요 없이 Amazon S3에 저장된 데이터에 직접 쿼리를 실행합니다. 또한, S3에 저장된 데이터와 Amazon Redshift에 저장된 데이터를 조인할 수 있습니다. 이를 보여주기 위해 event라는 일반 Redshift 테이블을 생성하고 이 테이블로 데이터를 로드합니다.\n 다음 명령을 실행하여 일반 Redshift 테이블을 생성합니다. event 테이블이 페이지 왼쪽의 테이블 목록에 표시됩니다.\nCREATE TABLE event( eventid INTEGER NOT NULL DISTKEY, venueid SMALLINT NOT NULL, catid SMALLINT NOT NULL, dateid SMALLINT NOT NULL SORTKEY, eventname VARCHAR(200), starttime TIMESTAMP )  INSERT-YOUR-REDSHIFT-ROLE을 이전 단계에서 생성한 redshift_role의 ARN 값을 대체하고 pgweb에서 이 명령을 실행하여 데이터를 events 테이블로 로드합니다.  약 30초 가량의 로딩 시간이 소요됩니다.\nCOPY event FROM 's3://id-redshift-uswest2/tickit/allevents_pipe.txt' IAM_ROLE 'INSERT-YOUR-REDSHIFT-ROLE' DELIMITER '|' TIMEFORMAT 'YYYY-MM-DD HH:MI:SS' REGION 'us-west-2'  다음 명령을 실행하여 event 데이터의 샘플을 확인합니다.\nSELECT * FROM event LIMIT 10  이제 이 새로운 event 테이블의 데이터 (Redshift 저장 데이터)와 외부 sales 테이블의 데이터 (S3 저장데이터)를 조인하는 쿼리를 실행할 수 있습니다.\n 다음의 명령을 통해 로컬 event 테이블과 외부 sales 테이블을 조인하여 상위 10개 이벤트의 총 매출을 확인합니다.\nSELECT TOP 10 spectrum.sales.eventid, SUM(spectrum.sales.pricepaid) FROM spectrum.sales, event WHERE spectrum.sales.eventid = event.eventid AND spectrum.sales.pricepaid \u0026gt; 30 GROUP BY spectrum.sales.eventid ORDER BY 2 DESC  이 쿼리는 가격이 30 USD 이상의 이벤트 별 (Redshift 저장 데이터) 로 그룹화된 총 매출 (S3 저장 데이터) 을 나열합니다.\n 다음의 명령을 실행하여 위의 쿼리에 대한 쿼리 플랜을 확인합니다. 이 쿼리 플랜은 Redshift가 해당 쿼리를 어떻게 실행할 지 보여줍니다. Amazon S3에 있는 데이터에 대해 S3 Seq Scan, S3 HashAggregate 및 S3 Query Scan 단계가 실행됩니다.\nEXPLAIN SELECT TOP 10 spectrum.sales.eventid, SUM(spectrum.sales.pricepaid) FROM spectrum.sales, event WHERE spectrum.sales.eventid = event.eventid AND spectrum.sales.pricepaid \u0026gt; 30 GROUP BY spectrum.sales.eventid ORDER BY 2 DESC   파티션된 데이터 사용 외부 테이블은 디렉터리로 사전에 파티션 될 수 있으며, 각 디렉터리는 데이터 하위집합을 포함합니다. 데이터를 파티션할 때 파티션 키를 필터링하여 Redshift Spectrum이 스캔하는 데이터의 양을 제한할 수 있습니다. 시간에 따라 데이터를 파티션하는 것이 일반적입니다. 예를 들어, 년, 월, 일 및 시간에 따라 파티션할 수 있습니다. 데이터가 여러 소스에서 수신되는 경우, 데이터 소스 식별자와 날짜로 파티션할 수 있습니다. 다음은 분할된 데이터를 보여주는 디렉터리 목록으로, 디렉터리에 월별로 파티션된 S3 파일 집합을 표시합니다. (참고: AWS Cli가 설치된 로컬 머신에서 확인 가능 합니다.)\n$ aws s3 ls s3://id-redshift-uswest2/tickit/spectrum/sales_partition/  PRE saledate=2008-01/ PRE saledate=2008-02/ PRE saledate=2008-03/ PRE saledate=2008-04/ PRE saledate=2008-05/ PRE saledate=2008-06/ PRE saledate=2008-07/ PRE saledate=2008-08/ PRE saledate=2008-09/ PRE saledate=2008-10/ PRE saledate=2008-11/  이제 이 데이터를 사용하는 외부 테이블을 정의 합니다. 다음 명령을 실행하여 파티션된 데이터에 따라 새로운 sales_partitioned 테이블을 정의합니다.\nCREATE EXTERNAL TABLE spectrum.sales_partitioned( salesid INTEGER, listid INTEGER, sellerid INTEGER, buyerid INTEGER, eventid INTEGER, dateid SMALLINT, qtysold SMALLINT, pricepaid DECIMAL(8,2), commission DECIMAL(8,2), saletime TIMESTAMP ) PARTITIONED BY (saledate DATE) ROW FORMAT DELIMITED FIELDS TERMINATED BY '|' STORED AS TEXTFILE LOCATION 's3://id-redshift-uswest2/tickit/spectrum/sales_partition/' TABLE PROPERTIES ('numRows'='172000')  (이 쿼리를 실행하면 화면에 응답이 표시되지 않지만, 테이블 정의가 생성됩니다.) salesdate 필드에 따라 테이블이 파티션됨을 Redshift Spectrum에 알려주는 문이 추가되었습니다. 그런 다음 Redshift Spectrum은 기존 파티션에 대한 정보를 받아야 어떤 디렉터리를 사용할 지 알 수 있습니다. 다음 명령을 실행하여 파티션을 추가합니다.\nALTER TABLE spectrum.sales_partitioned ADD PARTITION (saledate='2008-01-01') LOCATION 's3://id-redshift-uswest2/tickit/spectrum/sales_partition/saledate=2008-01/'; ALTER TABLE spectrum.sales_partitioned ADD PARTITION (saledate='2008-02-01') LOCATION 's3://id-redshift-uswest2/tickit/spectrum/sales_partition/saledate=2008-02/'; ALTER TABLE spectrum.sales_partitioned ADD PARTITION (saledate='2008-03-01') LOCATION 's3://id-redshift-uswest2/tickit/spectrum/sales_partition/saledate=2008-03/'; ALTER TABLE spectrum.sales_partitioned ADD PARTITION (saledate='2008-04-01') LOCATION 's3://id-redshift-uswest2/tickit/spectrum/sales_partition/saledate=2008-04/'; ALTER TABLE spectrum.sales_partitioned ADD PARTITION (saledate='2008-05-01') LOCATION 's3://id-redshift-uswest2/tickit/spectrum/sales_partition/saledate=2008-05/'; ALTER TABLE spectrum.sales_partitioned ADD PARTITION (saledate='2008-06-01') LOCATION 's3://id-redshift-uswest2/tickit/spectrum/sales_partition/saledate=2008-06/'; ALTER TABLE spectrum.sales_partitioned ADD PARTITION (saledate='2008-07-01') LOCATION 's3://id-redshift-uswest2/tickit/spectrum/sales_partition/saledate=2008-07/'; ALTER TABLE spectrum.sales_partitioned ADD PARTITION (saledate='2008-08-01') LOCATION 's3://id-redshift-uswest2/tickit/spectrum/sales_partition/saledate=2008-08/'; ALTER TABLE spectrum.sales_partitioned ADD PARTITION (saledate='2008-09-01') LOCATION 's3://id-redshift-uswest2/tickit/spectrum/sales_partition/saledate=2008-09/'; ALTER TABLE spectrum.sales_partitioned ADD PARTITION (saledate='2008-10-01') LOCATION 's3://id-redshift-uswest2/tickit/spectrum/sales_partition/saledate=2008-10/'; ALTER TABLE spectrum.sales_partitioned ADD PARTITION (saledate='2008-11-01') LOCATION 's3://id-redshift-uswest2/tickit/spectrum/sales_partition/saledate=2008-11/'; ALTER TABLE spectrum.sales_partitioned ADD PARTITION (saledate='2008-12-01') LOCATION 's3://id-redshift-uswest2/tickit/spectrum/sales_partition/saledate=2008-12/';  이제 특정 salesdate를 사용하는 모든 쿼리에서 해당 날짜와 관련된 디렉터리만 스캔합니다. 비교를 위해 2개의 서로 다른 데이터 소스에 쿼리를 실행합니다.\n 원래 sales 테이블에 다음 명령을 실행하고 실행에 걸리는 시간을 기록합니다.\nSELECT TOP 10 spectrum.sales.eventid, SUM(pricepaid) FROM spectrum.sales, event WHERE spectrum.sales.eventid = event.eventid AND pricepaid \u0026gt; 30 AND date_trunc('month', saletime) = '2008-12-01' GROUP BY spectrum.sales.eventid ORDER BY 2 DESC  파티션된 데이터에 다음의 명령을 실행하고 실행에 걸리는 시간을 기록합니다.\nSELECT TOP 10 spectrum.sales_partitioned.eventid, SUM(pricepaid) FROM spectrum.sales_partitioned, event WHERE spectrum.sales_partitioned.eventid = event.eventid AND pricepaid \u0026gt; 30 AND saledate = '2008-12-01' GROUP BY spectrum.sales_partitioned.eventid ORDER BY 2 DESC  두번째 쿼리가 더 빠르게 실행되는 것을 확인합니다. 이는 Amazon S3에서 읽는 데이터가 더 적기 때문입니다. 데이터 볼륨이 클수록 실행 속도의 차이가 더 분명해 집니다. (다만 본 예제와 같이 데이터량이 작은 경우 그 차이는 미비합니다.) 또한, Amazon S3에서 읽는 데이터량에 따라 Redshift Spectrum에 대한 요금이 부과되므로, 쿼리 실행 비용도 줄어듭니다.\n  파티션에 대한 정보는 SVV_EXTERNAL_PARTITIONS 시스템 뷰에서 확인할 수 있습니다. 1. 다음의 명령을 실행하여 sales_partitioned 테이블에 대한 파티션을 확인합니다.\n SELECT * FROM SVV_EXTERNAL_PARTITIONS WHERE tablename = 'sales_partitioned'   2. External tables에 대한 정보는 _SVV_EXTERNAL_COLOUMS 시스템 뷰에서 확인할 수 있습니다. 3. 다음의 명령을 실행하여 sales_partitioned 테이블에 정의된 열을 확인합니다.\n실습 완료 이번 실습을 완료하였습니다. 비용 발생을 최소화 하기 위하여 실습환경을 정리하십시오.\n© 2019 Amazon Web Services, Inc. 또는 자회사, All rights reserved. "
},
{
	"uri": "/analytics201/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/analytics201/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/analytics201/credits/",
	"title": "크레딧",
	"tags": [],
	"description": "",
	"content": " 컨텐츠 기여자  AWS CS Korea SA  hyouk@ kseongmo@ sehyul@   컨텐츠 웹 변환 기여자  AWS WWPS Korea SA  inhwan@   소프트웨어 기여자 오픈 소스 소프트웨어를 더 좋게 만들어 주시는 기여자 분들께 감사드립니다!\n.ghContributors{ display:flex; flex-flow: wrap; align-content: flex-start }\n.ghContributors \u0026gt; div{ width: 50% ; display: inline-flex; margin-bottom: 5px; } .ghContributors \u0026gt; div label{ padding-left: 4px ; } .ghContributors \u0026gt; div span{ font-size: x-small; padding-left: 4px ; }\n  @matcornic 154 commits \n @matalo33 39 commits \n @coliff 17 commits \n @lierdakil 16 commits \n @gwleclerc 13 commits \n @mdavids 10 commits \n @ozobi 5 commits \n @Xipas 5 commits \n @pdelaby 4 commits \n @Chris-Greaves 3 commits \n @mreithub 3 commits \n @massimeddu 3 commits \n @dptelecom 3 commits \n @willwade 3 commits \n @denisvm 2 commits \n @gpospelov 2 commits \n @jamesbooker 2 commits \n @tanzaho 2 commits \n @wikijm 2 commits \n @lfalin 2 commits \n @alexvargasbenamburg 1 commits \n @afs2015 1 commits \n @arifpedia 1 commits \n @berryp 1 commits \n @MrMoio 1 commits \n @ChrisLasar 1 commits \n @IEvangelist 1 commits \n @giuliov 1 commits \n @haitch 1 commits \n @ImgBotApp 1 commits \n @RealOrangeOne 1 commits \n @JohnBlood 1 commits \n @JohnAllen2tgt 1 commits \n @kamilchm 1 commits \n @lloydbenson 1 commits \n @massimocireddu 1 commits \n @sykesm 1 commits \n @nvasudevan 1 commits \n @654wak654 1 commits \n @PierreAdam 1 commits \n @ripienaar 1 commits \n @pocc 1 commits \n @EnigmaCurry 1 commits \n @taiidani 1 commits \n @exKAZUu 1 commits \n @Oddly 1 commits \n @shelane 1 commits \n @tedyoung 1 commits \n @Thiht 1 commits \n @editicalu 1 commits \n @fossabot 1 commits \n @kamar535 1 commits \n @nonumeros 1 commits \n @pgorod 1 commits \n @proelbtn 1 commits \n\n@vjeantet의 docdock, hugo-theme-learn의 분기에 대한 업적에 특별히 감사드립니다. 이 테마의 v2.0.0은 그의 작업으로 부터 영감 받았습니다.\n패키지와 라이브러리  mermaid - generation of diagram and flowchart from text in a similar manner as markdown font awesome - the iconic font and CSS framework jQuery - The Write Less, Do More, JavaScript Library lunr - Lunr enables you to provide a great search experience without the need for external, server-side, search services\u0026hellip; horsey - Progressive and customizable autocomplete component clipboard.js - copy text to clipboard highlight.js - Javascript syntax highlighter modernizr - A JavaScript toolkit that allows web developers to use new CSS3 and HTML5 features while maintaining a fine level of control over browsers that don\u0026rsquo;t support  도구  Netlify - Continuous deployement and hosting of this documentation Hugo  "
}]